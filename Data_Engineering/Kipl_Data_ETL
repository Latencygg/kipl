# 사용 라이브러리 : apache_beam, InteractiveRunner from apache_beam.runners.interactive.interactive_runner
apache_beam.runners.interactive.interactive_beam, pandas, re, OrderedDict from collections
glob, bigquery from google.cloud, service_account from google.oauth2, mysql.connector
uuid, binascii, storage from google.cloud, random, GoogleCloudOptions from apache_beam.options.pipeline_options
PipelineOptions, GoogleCloudOptions from apache_beam.options.pipeline_options, os

# 사용 API : Vertual AI's Jupyter Notebook, GCP, Apache Beam, Cloud Storage, Big Query, VM Instance, Data Flow


# 코드 이용시 주의사항 : GCP Vertual AI's Jupyter NoteBook으로 작업을 하여서, 파일 다운로드 path나 API's Key 경로는 local에서 진행시 각 체제의 맞게 변경 바람!!
========================================================================================================================================================================
========================================================================================================================================================================

import apache_beam as beam
from apache_beam.runners.interactive.interactive_runner import InteractiveRunner
import apache_beam.runners.interactive.interactive_beam as ib
import pandas as pd
import re
from collections import OrderedDict
import glob
from google.cloud import bigquery
from google.oauth2 import service_account
import mysql.connector
import uuid
import binascii
from google.cloud import storage
import random
from apache_beam.options.pipeline_options import GoogleCloudOptions
from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions
import os

def get_evaluation(x):
    temp_list = []
    def select_evaluation(x):
        select = 'select food_evaluation, food_name  from food_evaluation where person_id = UNHEX("{}")'
        select = select.format(x)
    
        cursor.execute(select)
        results = cursor.fetchall()
        return results
    try:
        connection = mysql.connector.connect(**db_config)
        cursor = connection.cursor()
        
        result = select_evaluation(x)
        
        
        connection.commit()
        for i in result:
            temp_list.append(i)
        
        return list(set(temp_list))
    except mysql.connector.Error as error:
        print("Error:", error)

    finally:
        if connection.is_connected():
            cursor.close()
            connection.close()

def food_dict():
    temp_board = bq_select('jnu-team-03-2.kipl_bigquery_database.menu_board')
    for i in temp_board:
        a = i[1]
    menu_board = []
    food = []
    different_food = []
    for m in a:
        menu_board.append(m['morning'])
        menu_board.append(m['lunch'])
        menu_board.append(m['dinner'])

    cleaned_flat_list = [x.strip() for x in menu_board]  
    menu_board = list(OrderedDict.fromkeys(cleaned_flat_list))

    temp_food = bq_select('jnu-team-03-2.kipl_bigquery_database.food')

    for i in temp_food:
        food.append(i[0])
    return food

def bq_select(x):
    temp_list = []
    query = 'select * from {}'
    query = query.format(x)
    
    query_job = client.query(query)
    return query_job

def get_ingredient():
    temp = []
    b_query = 'select a.food_name, b.ingredient_name from `jnu-team-03-2.kipl_bigquery_database.food` as a join `jnu-team-03-2.kipl_bigquery_database.food` as b on a.food_name = b.food_name'
    query_job = client.query(b_query)
    for i in query_job:
        temp.append(i)
    return temp

class Allergy_Value_DoFn():
    def __init__(self,food_value,uuid):
        self.uuid = uuid
        self.value = food_value[0]
        self.temp = None
        self.allergy = None
        self.allergy_food = None
        
    def process(self):
        temp = get_evaluation(self.uuid)  # get_evaluation 함수에 대한 구현이 필요함
        print(temp,'===================')
        for i in temp:
            if i[0] == 0:
                self.value[i[1]] = 0
            elif i[0] == 1:
                self.value[i[1]] /= 2
        
    def join_person_ingredient(self):
        temp = []
        select = 'select a.person_id, b.ingredient_name from personal_allergy a join ingredient b on a.allergy_name = b.allergy_name'
        connection = mysql.connector.connect(**db_config)
        cursor = connection.cursor()
        cursor.execute(select)
        results = cursor.fetchall()
        try:
            for i in results:
                temp.append((binascii.hexlify(i[0]), i[1]))
            self.temp = temp
        except mysql.connector.Error as error:
            print("Error:", error)
        finally:
            if connection.is_connected():
                cursor.close()
                connection.close()
                
    def find_UUID_allergy(self):
        allergy = []
        for i in self.temp:
            if self.uuid == i[0].decode('utf-8'):
                allergy.append(i[1])
        self.allergy = allergy
        
    def get_allergy_food(self):
        temp = []
        food = get_ingredient()
        for i in self.allergy:
            for j in food:
                if i in j[1]:
                    temp.append(j[0])
        self.allergy_food = list(set(temp))
        print(self.allergy_food)
    def result(self):
        for i in self.allergy_food:
            if i in self.value.keys():
                self.value[i] = 0
    
    def __call__(self, a):
        self.process()
        self.join_person_ingredient()
        self.find_UUID_allergy()
        self.get_allergy_food()
        self.result()
        yield self.value

def change_value_to_num(data):
    query = 'select * from `jnu-team-03-2.kipl_bigquery_database.food_category2`'
    query_job = client.query(query)

    category = []
    food_name = []
    cluster = []

    for i in query_job:
        category.append(i[0])
        food_name.append(i[1])
        cluster.append(i[2])


    table = {
        'category' : category,
        'food_name' : food_name,
        'cluster' : cluster
    }
    
    df = pd.DataFrame(table)
    
    cluster_dict = dict()

    cluster_dict = {x : [] for x in df['cluster']}
    for cluster,name in zip(df['cluster'],df['food_name']):
        cluster_dict[cluster].append(name)
    
    clustered_data = {}
    for i in cluster_dict:
        clustered_data[i] = []
        for j in data:
            if j in cluster_dict[i]:
                clustered_data[i].append(data[j])
    return clustered_data

def change_value_to_dict(data):
    query = 'select * from `jnu-team-03-2.kipl_bigquery_database.food_category2`'
    query_job = client.query(query)

    category = []
    food_name = []
    cluster = []

    for i in query_job:
        category.append(i[0])
        food_name.append(i[1])
        cluster.append(i[2])


    data = {
        'category' : category,
        'food_name' : food_name,
        'cluster' : cluster
    }
    
    df = pd.DataFrame(data)
    
    cluster_dict = dict()

    cluster_dict = {x : [] for x in df['cluster']}
    for cluster,name in zip(df['cluster'],df['food_name']):
        cluster_dict[cluster].append(name)
    return cluster_dict


def num_to_average(num_dict):
    result_dict = {}  # 새로운 결과 사전을 만듦
    for i in num_dict:
        denominator = len(num_dict[i]) - num_dict[i].count(0)
        # 리스트 내의 각 요소를 평균으로 변환하여 새 리스트에 저장
        new_values = [j/denominator if j != 0 else 0 for j in num_dict[i]]
        result_dict[i] = new_values  # 변환된 값을 새로운 사전에 저장
    return result_dict


class ProcessResultDoFn(beam.DoFn):
    def __init__(self):
        self.temp = []
        self.temp2 = []

    def process(self, element):
        self.temp.append(element)
        if len(self.temp) <= 1:
            self.temp = self.temp
        else:
            for i in self.temp:
                self.temp2.append(i)
            yield self.temp2
    
    
def choose_indices_with_probability(prob_list):
    query = 'SELECT * FROM `jnu-team-03-2.kipl_bigquery_database.menu_category` ORDER BY day ASC;'
    
    query_job = client.query(query)

    day = []
    morning = []
    lunch = []
    dinner = []

    for i in query_job:
        day.append(i[0])
        morning.append(i[1])
        lunch.append(i[2])
        dinner.append(i[3])


    data = {
        'day' : day,
        'morning' : morning,
        'lunch' : lunch,
        'dinner' : dinner
    }
    
    df_menu = pd.DataFrame(data)

    menu_dict_morning = {x : [] for x in range(df_menu['morning'].max()+1)}
    menu_dict_lunch = {x : [] for x in range(df_menu['morning'].max()+1)}
    menu_dict_dinner  = {x : [] for x in range(df_menu['morning'].max()+1)}
    
    
    
    menu_dict = dict()
    result = dict()
    list = []
    another_list = []
    num = 0
    if isinstance(prob_list[0][0][0],float) or isinstance(prob_list[0][0][0],int):
        list = prob_list[0]
        another_list = prob_list[1]
    else:
        list = prob_list[1]
        another_list = prob_list[0]

    for time in ['morning','lunch','dinner']:
        new_dict = {x : [] for x in range(len(prob_list[-1]))}
        df_menu_num = df_menu[time]
        
        for i in list:
            num_choices = len(df_menu_num[df_menu[time] == i])
            if all(p == 0 for p in list[i]):
                new_dict[i] = []
            else:    
                selected_indices = random.choices(range(len(list[i])), weights=list[i], k=num_choices)
                new_dict[i] = selected_indices
        menu_dict[time] = new_dict
    result_dict = {x : [] for x in range(len(prob_list[-1]))}
    
    for index in menu_dict['morning']:
        result_list = []
        for map_index in menu_dict['morning'][index]:
            result_list.append(another_list[index][map_index])
        result_dict[index] = result_list
    menu_dict['morning'] = result_dict
    
    result_dict = {x : [] for x in range(len(prob_list[-1]))}
    
    for index in menu_dict['lunch']:
        result_list = []
        for map_index in menu_dict['lunch'][index]:
            result_list.append(another_list[index][map_index])
        result_dict[index] = result_list
    menu_dict['lunch'] = result_dict
    
    result_dict = {x : [] for x in range(len(prob_list[-1]))}
    
    for index in menu_dict['dinner']:
        result_list = []
        for map_index in menu_dict['dinner'][index]:
            result_list.append(another_list[index][map_index])
        result_dict[index] = result_list
    menu_dict['dinner'] = result_dict
   
    
    return menu_dict

def Create_result(data):
    query = 'select * from `jnu-team-03-2.kipl_bigquery_database.menu_category` order by day'
    query_job = client.query(query)
    
    morning_menu = []
    lunch_menu = []
    dinner_menu = []
    
    results = query_job.result()
    
    for i in results:
        morning_menu.append(i[1])
        lunch_menu.append(i[2])
        dinner_menu.append(i[3])
    
    morning_result = []
    lunch_result = []
    dinner_result = []
    day = [x for x in range(1,181)]

    # for time in data['morning']:
    #     for index in data['morning'][time]:

    for menu in [morning_menu, lunch_menu, dinner_menu]:
        for i in menu:
            if menu == morning_menu:
                morning_result.append(data['morning'][i].pop())
            elif menu == lunch_menu:
                lunch_result.append(data['lunch'][i].pop())
            else:
                dinner_result.append(data['dinner'][i].pop())
     
    
    
    menu_boards = []
    for day, morning, lunch, dinner in zip(day,morning_result, lunch_result, dinner_result):
        menu_board = {
            'day': day,
            'morning': morning,
            'lunch': lunch,
            'dinner': dinner
        }
        menu_boards.append(menu_board)
        
    return menu_boards


def transform_data(element, person_id):
    
    return {
        'person_id': person_id,
        'menu': element
    }

class WriteToBigQuery(beam.PTransform):
    def __init__(self, table_name, gcs_temp_location):
        self.table_name = table_name
        self.gcs_temp_location = gcs_temp_location
    def expand(self, pcoll):
        return (
            pcoll
            | 'Write to BigQuery' >> beam.io.WriteToBigQuery(
                self.table_name,
                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,
                custom_gcs_temp_location=self.gcs_temp_location
            )
        )
    
def person_id_exists(element, person_id):
    credentials = service_account.Credentials.from_service_account_file(KEY_PATH)
    # 빅쿼리 클라이언트 객체 생성
    query = f"SELECT COUNT(*) FROM `jnu-team-03-2.kipl_bigquery_database.menu_board` WHERE person_id = '{person_id}'"
    query_job = client.query(query)
    results = query_job.result()
    for i in results:
        if i[0] == 0:
            print('create table')
            return element
        else:
            print('table aleady exist')



# 파이프라인 정의
def run_pipeline():
    pipeline_options = PipelineOptions()
    google_cloud_options = pipeline_options.view_as(GoogleCloudOptions)
    google_cloud_options.temp_location = 'gs://temp_bucket_01110/menu_board'

    # Beam 파이프라인 옵션 설정
    options = PipelineOptions()
    options.view_as(beam.options.pipeline_options.GoogleCloudOptions).project = 'jnu-idv-05'
    options.view_as(beam.options.pipeline_options.GoogleCloudOptions).region = 'us'
    options.view_as(beam.options.pipeline_options.GoogleCloudOptions).staging_location = 'gs://temp_bucket_01110/staging_location'
    options.view_as(beam.options.pipeline_options.GoogleCloudOptions).temp_location = 'gs://temp_bucket_01110/temp_location'

    # b'48420e96a9dc49cdbb0fe04418112cdb'
    # b'e14e4fdfa7b94e2ab1310fe101c6567a'
    # b'10c806816107447eb05e22eb4e90cf80'
    with beam.Pipeline(options=options) as p:
        temp = []
    
        food = food_dict()
        # Assuming input_data is read from a source
        input_data = [{key: 1 for key in food}]
        uuid = b'10c806816107447eb05e22eb4e90cf80'
        table_name = 'jnu-team-03-2.kipl_bigquery_database.menu_board'
        gcs_temp_location = 'gs://temp_bucket_01110/menu_board'

        # Apply the custom DoFn to process and change values
        changed_values = (
            p | "Create input collection" >> beam.Create(input_data)
              | "Change values" >> beam.ParDo(Allergy_Value_DoFn(input_data, uuid.decode('utf-8')))
        )
        changed_values2 = (
            changed_values | "value to dict" >> beam.Map(change_value_to_dict)
        )

        changed_values3 = (
            changed_values | "dict to num" >> beam.Map(change_value_to_num)
                           | "num to average" >> beam.Map(num_to_average)
        )

        # Transform and process data as needed
        result = (
            (changed_values2, changed_values3)
            | "Flatten" >> beam.Flatten()
            | "Process Result" >> beam.ParDo(ProcessResultDoFn())
            | "random result" >> beam.Map(choose_indices_with_probability)
            | "Create result" >> beam.Map(Create_result)
            | 'Transform Data' >> beam.Map(transform_data, uuid.decode('utf-8'))
            | "Filter existing person_id" >> beam.Filter(person_id_exists,uuid.decode('utf-8')
            )
            | "Write to BigQuery" >> beam.io.WriteToBigQuery(
                table_name,
                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,
                custom_gcs_temp_location=gcs_temp_location
            )
        )

if __name__ == "__main__":
    run_pipeline()
